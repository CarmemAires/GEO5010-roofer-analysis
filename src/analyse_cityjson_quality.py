#!/usr/bin/env python3
"""
analyze_cityjson_quality.py

Performs a comprehensive quality analysis of CityJSON datasets per typology.
Includes:

- Parsing CityJSON files to extract building attributes and val3dity validation codes
- Aggregating statistics per typology (RMSE, point density, no-data fraction, extrusion modes)
- Processing corresponding GPKG footprints to calculate overlaps and quality indicators
- Generating detailed visualizations (doughnut charts, radar charts, RMSE ranges, validation error bars)
- Producing summary JSON and CSV outputs for reporting

Inputs:
- CityJSON files (*.city.json) in a specified directory
- Corresponding GPKG footprint files (pattern: *_<typology>_<footprints_basename>.gpkg)

Outputs:
- Aggregated statistics JSON: aggregated_statistics_by_typology.json
- Summary CSV: summary_statistics.csv
- Visualizations per typology (plots saved as PNG)
- Updated GPKGs with quality indicator columns

Requirements (Python packages):
- numpy
- pandas
- geopandas
- shapely
- matplotlib
- seaborn
- tqdm
- fiona
- pyproj
- rtree 

Configuration:

- DEST_DIR: Path to the main directory containing CityJSON and GPKG files generated by process_roofer_parallel_tuned.py.
            Example: DEST_DIR = Path("data/cityjson_analysis")

- FOOTPRINTS_BASE_NAME: Base name of the footprints GPKG files used in process_roofer_parallel_tuned.py FOOTPRINTS_GPKG.gpkg.
            Example: FOOTPRINTS_BASE_NAME = "FOOTPRINTS_GPKG"
            
Author: Carmem E. F. Aires
Date: 2026-01-12
"""
from __future__ import annotations

import logging
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
import re
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns
from shapely.geometry import shape
from tqdm import tqdm
import textwrap
from matplotlib.patches import Patch
from collections import Counter

# ---------- CONFIG ----------

DEST_DIR = Path(r"data/output") # Set the path to directory to store outputs
FOOTPRINTS_BASE_NAME = "footprints" # Set the base name of the footprints GPKG files


OUTPUT_BASE_DIR = DEST_DIR 
OUTPUT_STATS_DIR = OUTPUT_BASE_DIR / "quality_statistics"
OUTPUT_PLOTS_DIR = OUTPUT_BASE_DIR / "quality_plots"
# ----------------------------

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

def safe_mkdir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def typology_to_key(typology: str) -> str:
    return (
        typology
        .lower()
        .replace("-", "_")
        .replace(" ", "_")
    )

def parse_cityjson_filename(filename: str) -> Optional[Tuple[str, str]]:
    """Parse filename like 'MDS_color_4436-153_Suburban_single_family_house.city.json'
    Returns (prefix, building_type) where building_type has underscores replaced with spaces."""
    
    if not filename.endswith(".city.json"):
        return None
    
    # Remove the file extension
    stem = filename.replace(".city.json", "")
    
    # Split by underscores
    parts = stem.split("_")
    
    if len(parts) < 3:
        return None
    
    # Find the part that contains the ID
    for i, part in enumerate(parts):
        if '-' in part and part.replace('-', '').isdigit():

            prefix = "_".join(parts[:i])  # Join back with underscores
            building_type_parts = parts[i+1:]
            
            if building_type_parts:
                # Join building type with spaces instead of underscores
                building_type = " ".join(building_type_parts)
                return prefix, building_type
            break
    
    # Fallback to original logic if ID pattern not found
    parts = stem.rsplit("_", 1)
    if len(parts) == 2:
        return parts[0], parts[1]
    
    return None

def load_cityjson(filepath: Path) -> Optional[Dict[str, Any]]:
    """Load and parse a CityJSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logging.error(f"Failed to load {filepath}: {e}")
        return None

def parse_val3dity_codes(val3dity_data: Any) -> List[int]:
    """
    Parse val3dity error codes from various possible formats.
    Handles: "[102]", "[102, 103]", "102", 102, [102], etc.
    """
    if val3dity_data is None:
        return []
    
    # If it's already a list of integers, return it
    if isinstance(val3dity_data, list):
        return [int(x) for x in val3dity_data if str(x).strip().isdigit()]
    
    # If it's a string, try to extract numbers
    if isinstance(val3dity_data, str):
        # Remove brackets and split by commas
        cleaned = val3dity_data.strip('[]')
        if not cleaned:
            return []
        
        # Extract all numbers from the string
        numbers = re.findall(r'\d+', cleaned)
        return [int(num) for num in numbers if num.strip().isdigit()]
    
    # If it's a single integer
    if isinstance(val3dity_data, int):
        return [val3dity_data]
    
    # If it's a single float
    if isinstance(val3dity_data, float):
        return [int(val3dity_data)]
    
    logging.warning(f"Unexpected val3dity data type: {type(val3dity_data)} = {val3dity_data}")
    return []

# Convert numeric values to float without capping
def convert_numeric_value(value):
    if value is None:
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        logging.warning(f"Invalid numeric value: {value}")
        return None



def extract_building_attributes(cityjson_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    Extract relevant attributes from all buildings in a CityJSON file.
    Returns dictionary mapping building ID (uid/FEATID/fid) to building attributes.
    """
    buildings = {}
    city_objects = cityjson_data.get("CityObjects", {})
    
    # Debug counters
    total_buildings = 0
    buildings_with_id = 0
    buildings_without_id = 0
    duplicate_ids = 0
    
    for obj_id, obj_data in city_objects.items():
        if obj_data.get("type") == "Building":
            total_buildings += 1
            attrs = obj_data.get("attributes", {})
            
            
            # Get building ID - check multiple possible attribute names
          
            building_id = (attrs.get("FEATID") or 
                          attrs.get("featid") or 
                          attrs.get("fid") or
                          attrs.get("id"))
            
            if building_id is None:
                buildings_without_id += 1
                continue
            
            buildings_with_id += 1
            
            # Check for duplicate IDs
            id_str = str(building_id)
            if id_str in buildings:
                duplicate_ids += 1
                logging.warning(f"Duplicate ID found: {id_str} for building {obj_id}")
                # Create a unique key by appending the object ID
                unique_key = f"{id_str}_{obj_id}"
            else:
                unique_key = id_str
            
            # Parse val3dity codes properly
            val3dity_raw = attrs.get("rf_val3dity_lod22")
            val3dity_codes = parse_val3dity_codes(val3dity_raw)
            
            
            
            building_info = {
                "id": obj_id,
                "building_id": id_str,  # Store the actual ID value
                "rf_rmse_lod22": convert_numeric_value(attrs.get("rf_rmse_lod22")),
                "rf_pt_density": convert_numeric_value(attrs.get("rf_pt_density")),
                "rf_nodata_frac": convert_numeric_value(attrs.get("rf_nodata_frac")),
                "rf_nodata_r": convert_numeric_value(attrs.get("rf_nodata_r")),
                "rf_val3dity_lod22": val3dity_codes,
                "rf_extrusion_mode": attrs.get("rf_extrusion_mode"),
                "rf_pointcloud_unusable": attrs.get("rf_pointcloud_unusable"),
            }
            
            buildings[unique_key] = building_info
    
    # Debug logging
    logging.info(f"Building extraction: {total_buildings} total, {buildings_with_id} with ID, {buildings_without_id} without ID, {duplicate_ids} duplicates")
    
    if duplicate_ids > 0:
        logging.warning(f"Found {duplicate_ids} duplicate IDs - used object IDs to create unique keys")
    
    return buildings

def aggregate_building_data(typology_buildings: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
    """Aggregate all buildings from multiple files per typology."""
    aggregated = {}
    for typology, buildings_list in typology_buildings.items():
        # Flatten list of building dicts into single list
        all_buildings = []
        total_from_files = 0
        
        for buildings_dict in buildings_list:
            all_buildings.extend(buildings_dict.values())
            total_from_files += len(buildings_dict)
        
        aggregated[typology] = all_buildings
        
        # Debug: Check if we lost any buildings during aggregation
        if len(all_buildings) != total_from_files:
            logging.warning(f"Typology '{typology}': Aggregation mismatch - {len(all_buildings)} vs {total_from_files} expected")
        
        logging.info(f"Typology '{typology}': Aggregated {len(all_buildings)} buildings from {len(buildings_list)} files")
    
    return aggregated


def calculate_statistics(buildings: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Calculate comprehensive statistics for building attributes."""
    if not buildings:
        return {}
    
    total = len(buildings)

    stats = {
        "total_buildings": total,
    }
    
    # Debug: Count extrusion modes with detailed logging
    extrusion_modes = defaultdict(int)
    for b in buildings:
        mode = b["rf_extrusion_mode"]
        extrusion_modes[str(mode)] += 1
    
    logging.info(f"Extrusion mode distribution: {dict(extrusion_modes)}")
    
    # Numeric attributes statistics without capping
    numeric_attrs = ["rf_rmse_lod22", "rf_pt_density", "rf_nodata_frac", "rf_nodata_r"]
    
    for attr in numeric_attrs:
        values = [b[attr] for b in buildings if b[attr] is not None and isinstance(b[attr], (int, float)) and b[attr] != 0]
        
        if values:
            stats[f"{attr}_min"] = float(np.min(values))
            stats[f"{attr}_max"] = float(np.max(values))
            stats[f"{attr}_avg"] = float(np.mean(values))
            stats[f"{attr}_std"] = float(np.std(values))
            stats[f"{attr}_median"] = float(np.median(values))  
        else:
            stats[f"{attr}_min"] = 0.0
            stats[f"{attr}_max"] = 0.0
            stats[f"{attr}_avg"] = 0.0
            stats[f"{attr}_std"] = 0.0

    
    # rf_val3dity_lod22 statistics 
    valid_count = 0
    invalid_count = 0
    for b in buildings:
        val3dity_codes = b["rf_val3dity_lod22"]
        if len(val3dity_codes) == 0:
            valid_count += 1
        else:
            invalid_count += 1
    
    stats["rf_val3dity_valid_count"] = valid_count
    stats["rf_val3dity_invalid_count"] = invalid_count
    stats["rf_val3dity_valid_ratio"] = valid_count / len(buildings) if buildings else 0
    
    # Count by validation error codes 
    error_codes = defaultdict(int)
    for b in buildings:
        val3dity_codes = b["rf_val3dity_lod22"]
        for code in val3dity_codes:
            error_codes[str(code)] += 1
    
    stats["rf_val3dity_error_codes"] = dict(error_codes)
    
    # Log error code summary
    if error_codes:
        logging.info(f"Validation error codes found: {dict(error_codes)}")
    
    # rf_extrusion_mode statistics 
    
    # extrusion_modes already counted above; keys are strings
    stats["rf_extrusion_mode_counts"] = dict(extrusion_modes)

    # Explicit counts based on spec
    standard_count = extrusion_modes.get("standard", 0)
    lod11_fallback_count = extrusion_modes.get("lod11_fallback", 0)
    skip_count = extrusion_modes.get("skip", 0)

    stats["rf_extrusion_standard_count"] = standard_count
    stats["rf_extrusion_lod11_fallback_count"] = lod11_fallback_count
    stats["rf_extrusion_skip_count"] = skip_count

    stats["rf_extrusion_standard_ratio"] = standard_count / total
    stats["rf_extrusion_lod11_fallback_ratio"] = lod11_fallback_count / total
    stats["rf_extrusion_skip_ratio"] = skip_count / total

    # Debug unexpected or invalid modes
    for mode, count in extrusion_modes.items():
        if mode not in ["standard", "lod11_fallback", "skip"]:
            logging.warning(
                f"Unexpected rf_extrusion_mode '{mode}' found {count} times"
            )

    if standard_count + lod11_fallback_count + skip_count != total:
        logging.warning(
            "Some buildings have missing or invalid rf_extrusion_mode values"
        )
    
    # rf_pointcloud_unusable statistics - improved counting
    unusable_count = sum(
        str(b.get("rf_pointcloud_unusable")).lower() == "true"
        for b in buildings
    )
        
    stats["rf_pointcloud_unusable_count"] = unusable_count
    stats["rf_pointcloud_unusable_ratio"] = unusable_count / total if buildings else 0
    
        # ---- foot_foot_overlap statistics ----
    # Note: This can only be calculated after GPKG processing
    # This adds a placeholder and update it later
    stats["rf_overlapping_count"] = 0
    stats["rf_overlapping_ratio"] = 0.0
    
    return stats

def calculate_footprint_overlaps(gpkg_path: Path) -> gpd.GeoDataFrame:
    """Calculate overlapping areas between footprints in a GPKG."""
    logging.info(f"Calculating footprint overlaps for {gpkg_path.name}")
    gdf = gpd.read_file(gpkg_path)
    
    # Initialize overlap column
    gdf["foot_foot_overlap"] = 0.0
    
    # Create spatial index for efficiency
    sindex = gdf.sindex
    
    for idx, row in tqdm(gdf.iterrows(), total=len(gdf), desc="Checking overlaps"):
        # Find potential intersecting geometries
        possible_matches_idx = list(sindex.intersection(row.geometry.bounds))
        possible_matches = gdf.iloc[possible_matches_idx]
        
        # Calculate actual overlaps
        overlap_area = 0.0
        for other_idx, other_row in possible_matches.iterrows():
            if idx == other_idx:
                continue

            intersection = row.geometry.intersection(other_row.geometry)

            if row.geometry.overlaps(other_row.geometry):
                overlap_area += row.geometry.intersection(other_row.geometry).area  
        
        gdf.at[idx, "foot_foot_overlap"] = overlap_area
    
    logging.info(f"Overlap calculation complete: {(gdf['foot_foot_overlap'] > 0).sum()} overlapping footprints")
    return gdf


def calculate_overlap_statistics_from_gpkg(gdf: gpd.GeoDataFrame) -> Dict[str, Any]:
    """Calculate overlap statistics from a GeoDataFrame with foot_foot_overlap column."""
    total = len(gdf)
    
    # Count buildings with any overlap (overlap > tolerance)
    overlapping_count = (gdf["foot_foot_overlap"] > 1e-6).sum()
    overlapping_ratio = overlapping_count / total if total > 0 else 0
    
    return {
        "rf_overlapping_count": int(overlapping_count),
        "rf_overlapping_ratio": float(overlapping_ratio),
    }


def calculate_quality_indicator(gdf: gpd.GeoDataFrame, buildings_dict: Dict[str, Dict[str, Any]]) -> gpd.GeoDataFrame:
    """
    Calculate quality indicator for each building in the GeoDataFrame.

    """
    logging.info(f"Calculating quality indicator for {len(gdf)} features")
    
    # Initialize columns
    gdf["rf_val3dity_lod22_valid"] = None
    gdf["rf_qualityindicator"] = False
    
    # Find the ID column in the GPKG
    featid_col = None
    available_cols = list(gdf.columns)
    
    for col in ["cityjson_id", "uid", "FEATID", "featid", "fid", "FID", "id"]:
        if col in gdf.columns:
            featid_col = col
            logging.info(f"Using '{col}' as ID column")
            break
    
    if featid_col is None:
        logging.error(f"No ID column found in GPKG! Available columns: {available_cols}")
        return gdf
    
    # Debug: Show sample IDs from both sources
    sample_gpkg_ids = gdf[featid_col].head(5).tolist()
    sample_dict_ids = list(buildings_dict.keys())[:5]
    logging.info(f"Sample GPKG IDs: {sample_gpkg_ids}")
    logging.info(f"Sample buildings_dict keys: {sample_dict_ids}")
    
    matched_count = 0
    quality_pass_count = 0
    unmatched_ids = []
    extrusion_skip_failures = 0
    
    for idx, row in gdf.iterrows():
        featid = str(row[featid_col])
        
        # Try to find the building in the dictionary
        if featid not in buildings_dict:
            if len(unmatched_ids) < 10:  # Only store first 10 for debugging
                unmatched_ids.append(featid)
            continue
        
        attrs = buildings_dict[featid]
        
        # Check validation status (valid = no error codes)
        val3dity_valid = len(attrs["rf_val3dity_lod22"]) == 0
        
        # Check overlap status (should be near zero)
        overlap_ok = row["foot_foot_overlap"] < 1e-6

        extrusion_mode = attrs.get("rf_extrusion_mode")
        extrusion_ok = extrusion_mode != "skip"
        
        # Quality indicator passes if both conditions are met
        quality_ok = overlap_ok and val3dity_valid and extrusion_ok
        
        # Update the GeoDataFrame
        gdf.at[idx, "rf_val3dity_lod22_valid"] = val3dity_valid
        gdf.at[idx, "rf_qualityindicator"] = quality_ok
        
        
        # Debug output for first 5 matches
        if matched_count < 5:
            print(
                f"DEBUG Match #{matched_count + 1}:",
                f"ID={featid}",
                f"overlap={row['foot_foot_overlap']:.6f}",
                f"overlap_ok={overlap_ok}",
                f"val3dity_codes={attrs['rf_val3dity_lod22']}",
                f"val3dity_valid={val3dity_valid}",
                f"extrusion_mode={extrusion_mode}",
                f"extrusion_ok={extrusion_ok}",
                f"quality_ok={quality_ok}"
            )
        
        matched_count += 1
        if quality_ok:
            quality_pass_count += 1
        if not extrusion_ok:
            extrusion_skip_failures += 1
        
    
    # Summary logging
    logging.info(f"✓ Matched {matched_count}/{len(gdf)} features ({matched_count/len(gdf)*100:.1f}%)")
    logging.info(f"✗ Quality failed due to extrusion skip: {extrusion_skip_failures}")
    if matched_count > 0:
        logging.info(f"✓ Quality pass: {quality_pass_count}/{matched_count} ({quality_pass_count/matched_count*100:.1f}%)")
    else:
        logging.warning("⚠️  NO MATCHES FOUND! Check if ID columns match between GPKG and CityJSON")
    
    if unmatched_ids:
        logging.warning(f"⚠️  Sample unmatched IDs from GPKG: {unmatched_ids[:5]}")
    
    return gdf


def plot_validation_errors(ax, error_codes, max_error):
    codes = list(error_codes.keys())
    counts = list(error_codes.values())
    
    # Assign colors based on error code category
    colors = [get_val3dity_color(code) for code in codes]

    bars = ax.bar(codes, counts, color=colors, edgecolor="black")

    # Round max_error to next multiple of 5
    rounded_max = int(np.ceil(max_error / 5) * 5)
    
    # round to next multiple of 5
    ax.set_ylim(0, rounded_max)


    step = 5
    yticks = np.arange(0, rounded_max + step, step)
    ax.set_yticks(yticks)

    for bar, count in zip(bars, counts):
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            count,
            str(count),
            ha="center",
            va="bottom",
            fontsize=11
        )

    wrapped = ['\n'.join(textwrap.wrap(c, 12)) for c in codes]
    ax.set_xticklabels(wrapped, rotation=0, ha="center")

    # Create legend with color categories
    category_labels = {
        '#FFD6D5': '10x - LinearRing level',
        '#7BD273': '20x - Polygon level',
        '#F4B368': '30x - Shell level',
        '#00D5FF': '40x - Solid level',
        '#BFFFBF': '50x - Solid intersections level',
        '#D6A979': '60x - CityGML Objects',
        "#FFBFFF": '70x - IndoorGML Objects',
        '#FFFF80': '90x - Other errors',
    }
    
    # Only show categories that are present in the data
    present_colors = set(colors)
    legend_elements = [
        Patch(facecolor=color, label=label)
        for color, label in category_labels.items()
        if color in present_colors
    ]
    
    ax.legend(
        handles=legend_elements,
        title="Error Categories",
        bbox_to_anchor=(1.02, 1),
        loc="upper left"
    )



def get_val3dity_color(code):
    """Get color based on val3dity error code category."""
    code_str = str(code)
    
    # Color palette for each error category (10x, 20x, 30x, etc.)
    color_map = {
        '1': '#FFD6D5',  # 10x - LinearRing level
        '2': '#7BD273',  # 20x - Polygon level
        '3': '#F4B368',  # 30x - Shell level
        '4': '#00D5FF',  # 40x - Solid level
        '5': '#BFFFBF',  # 50x - Solid intersections level
        '6': '#D6A979',  # 60x - CityGML Objects
        '7': '#FFBFFF',  # 70x - IndoorGML Objects
        '9': '#FFFF80',  # 90x - Other errors
    }
    
    # Get first digit of error code
    if code_str and code_str[0] in color_map:
        return color_map[code_str[0]]
    
    return '#34495e'  # Default dark gray for unknown codes


def create_typology_visualizations(all_stats: Dict[str, Dict[str, Any]], output_dir: Path) -> None:
    

    logging.info("Creating redesigned visualizations...")
    output_dir.mkdir(parents=True, exist_ok=True)

    sns.set_style("white")
    plt.rcParams.update({"font.size": 13})

    DARK = "#334E54"
    POSITIVE = "#72A0AA"
    NEGATIVE = "#C4D7DB"

    typologies = list(all_stats.keys())

    # ---------------------------------------------------------
    # Helper functions
    # ---------------------------------------------------------
    def doughnut_chart(value_pct, label, title, filename):
        fig, ax = plt.subplots(figsize=(6, 6))
        ax.pie(
            [value_pct, 100 - value_pct],
            startangle=90,
            colors=[POSITIVE, NEGATIVE],
            wedgeprops=dict(width=0.4, edgecolor="white"),
        )
        ax.text(0, 0, f"{value_pct:.1f}%", ha="center", va="center",
                fontsize=28, fontweight="bold", color=DARK)
        ax.set_title(title, fontweight="bold")
        ax.axis("equal")
        plt.savefig(filename, dpi=300, bbox_inches="tight")
        plt.close()

    # ---------------------------------------------------------
    # FIRST: Calculate the global max error across ALL typologies
    # ---------------------------------------------------------
    max_error = 0
    for stats in all_stats.values():
        error_codes = stats.get("rf_val3dity_error_codes", {})
        if error_codes:
            local_max = max(error_codes.values())
            if local_max > max_error:
                max_error = local_max


    # ---------------------------------------------------------
    # Per typology charts
    # ---------------------------------------------------------
    for typology, stats in all_stats.items():
        tname = typology.title()

        # Point cloud usability
        doughnut_chart(
            (1 - stats["rf_pointcloud_unusable_ratio"]) * 100,
            "Usable",
            f"Usable Point Cloud – {tname}",
            output_dir / f"{typology}_pointcloud_usable.png"
        )

        # Extrusion success
        doughnut_chart(
            (1 - stats["rf_extrusion_skip_ratio"]) * 100,
            "Successful",
            f"Successful Extrusion – {tname}",
            output_dir / f"{typology}_extrusion_success.png"
        )

        # Overlapping geometries
        doughnut_chart(
            (1 - stats.get("rf_overlapping_ratio", 0)) * 100,
            "Non-overlapping",
            f"Non-overlapping Footprints – {tname}",
            output_dir / f"{typology}_non_overlapping.png"
        )

        # Validation ratio (valid 3D geometry)
        doughnut_chart(
            stats["rf_val3dity_valid_ratio"] * 100,
            "Valid",
            f"Valid 3D Geometry – {tname}",
            output_dir / f"{typology}_validation_valid.png"
        )

        # Quality indicator exploded pie
        fig, ax = plt.subplots(figsize=(6, 6))
        qpos = stats.get("rf_qualityindicator_ratio", 0) * 100
        ax.pie(
            [qpos, 100 - qpos],
            explode=(0.08, 0),
            labels=["Pass", "Fail"],
            colors=[POSITIVE, NEGATIVE],
            pctdistance=0.7,
            labeldistance=1.05, 
            autopct="%1.1f%%",
            radius=0.9,
            startangle=90,
            textprops={"fontweight": "bold", "fontsize": 16},
        )
        ax.set_title(f"Quality Indicator – {tname}", fontweight="bold", pad=20)
        ax.axis("equal")
        plt.savefig(output_dir / f"{typology}_quality_indicator.png", dpi=300, bbox_inches="tight")
        plt.close()

        # -----------------------------------------------------
        # Validation error bar chart (uses global max_error)
        # -----------------------------------------------------
        error_codes = stats.get("rf_val3dity_error_codes", {})
        
        fig, ax = plt.subplots(figsize=(9, 5))
        if error_codes:
            plot_validation_errors(ax, error_codes, max_error)  # uses global max for y-axis
        else:
            ax.text(0.5, 0.5, "No validation errors",
                    ha="center", va="center", fontsize=14, fontweight="bold")
            ax.set_xticks([])
            ax.set_yticks([])
        
        ax.set_title(f"Validation Error Codes – {tname}", fontweight="bold")
        ax.set_ylabel("Count")
        plt.subplots_adjust(right=0.75, bottom=0.28)
        plt.savefig(output_dir / f"{typology}_validation_errors.png", dpi=300)
        plt.close()



    # ---------------------------------------------------------
    # RMSE range plot 
    # ---------------------------------------------------------
    rmse_rows = []

    for typ, stats in all_stats.items():
        if stats.get("rf_rmse_lod22_avg") is not None:
            rmse_rows.append({
                "Typology": typ,
                "Min": stats["rf_rmse_lod22_min"],
                "Avg": stats["rf_rmse_lod22_avg"],
                "Max": stats["rf_rmse_lod22_max"],
                "Median": stats["rf_rmse_lod22_median"],
            })

    if rmse_rows:
        df = pd.DataFrame(rmse_rows)

        fig, ax = plt.subplots(figsize=(10, 6))

        y = np.arange(len(df))

        # Min–Max range
        ax.hlines(
            y=y,
            xmin=df["Min"],
            xmax=df["Max"],
            color=NEGATIVE,
            linewidth=4,
            label="Min–Max range"
        )

        # Average point
        ax.scatter(
            df["Avg"],
            y,
            color=POSITIVE,
            s=120,
            zorder=3,
            label="Average"
        )

        ax.scatter(
            df["Median"],
            y,
            color="orange",  
            marker='D',      
            s=100,
            zorder=4,
            label="Median"
        )

        # Add labels for Min, Avg, Max
        for i, row in df.iterrows():
            # ax.text(row["Min"], i + 0.1, f'{row["Min"]:.2f}', ha='right', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Avg"], i + 0.1, f'{row["Avg"]:.2f}', ha='center', va='bottom', fontsize=10, color=POSITIVE, fontweight='bold')
            # ax.text(row["Max"], i + 0.1, f'{row["Max"]:.2f}', ha='left', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Median"], i - 0.15, f'{row["Median"]:.2f}', ha='center', va='top', fontsize=10, color='orange', fontweight='bold')

        ax.set_yticks(y)
        ax.set_yticklabels(df["Typology"])
        ax.set_xlabel("RMSE (m)")
        ax.set_title("RMSE LoD 2.2 (Min–Avg–Max)", fontweight="bold")
        ax.grid(axis="x", alpha=0.3)
        ax.legend()

        plt.tight_layout()
        plt.savefig(output_dir / "rmse_lod22_range.png", dpi=300)
        plt.close()

    rf_nodata_rows = []
    for typ, stats in all_stats.items():
        if stats.get("rf_nodata_r_avg") is not None:
            rf_nodata_rows.append({
                "Typology": typ,
                "Min": stats["rf_nodata_r_min"],
                "Avg": stats["rf_nodata_r_avg"],
                "Max": stats["rf_nodata_r_max"],
                "Median": stats["rf_nodata_r_median"],
            })
    
    if rf_nodata_rows: 
        df = pd.DataFrame(rf_nodata_rows)

        fig, ax = plt.subplots(figsize=(10, 6))

        y = np.arange(len(df))

        # Min–Max range
        ax.hlines(
            y=y,
            xmin=df["Min"],
            xmax=df["Max"],
            color=NEGATIVE,
            linewidth=4,
            label="Min–Max range"
        )

        # Average point
        ax.scatter(
            df["Avg"],
            y,
            color=POSITIVE,
            s=120,
            zorder=3,
            label="Average"
        )

        ax.scatter(
            df["Median"],
            y,
            color="orange",  
            marker='D',      # Diamond marker
            s=100,
            zorder=4,
            label="Median"
        )

        # Add labels for Min, Avg, Max
        for i, row in df.iterrows():
            # ax.text(row["Min"], i + 0.1, f'{row["Min"]:.1f}m', ha='right', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Avg"], i + 0.1, f'{row["Avg"]:.1f}m', ha='center', va='bottom', fontsize=10, color=POSITIVE, fontweight='bold')
            # ax.text(row["Max"], i + 0.1, f'{row["Max"]:.1f}m', ha='left', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Median"], i - 0.15, f'{row["Median"]:.1f}m', ha='center', va='top', fontsize=10, color='orange', fontweight='bold')

        ax.set_yticks(y)
        ax.set_yticklabels(df["Typology"])
        ax.set_xlabel("No-data Radius (m)")
        ax.set_title("No-data Radius (Min–Avg–Max)", fontweight="bold")
        ax.grid(axis="x", alpha=0.3)
        ax.legend()

        plt.tight_layout()
        plt.savefig(output_dir / "no_data_radius_range.png", dpi=300)
        plt.close()

    no_data_rows = []

    for typ, stats in all_stats.items():
        if stats.get("rf_nodata_frac_avg") is not None:
            no_data_rows.append({
                "Typology": typ,
                "Min": stats["rf_nodata_frac_min"] * 100,
                "Avg": stats["rf_nodata_frac_avg"] * 100,
                "Max": stats["rf_nodata_frac_max"] * 100,
                "Median": stats["rf_nodata_frac_median"] * 100,
            })
    if no_data_rows:
        df = pd.DataFrame(no_data_rows)

        fig, ax = plt.subplots(figsize=(10, 6))

        y = np.arange(len(df))

        # Min–Max range
        ax.hlines(
            y=y,
            xmin=df["Min"],
            xmax=df["Max"],
            color=NEGATIVE,
            linewidth=4,
            label="Min–Max range"
        )

        # Average point
        ax.scatter(
            df["Avg"],
            y,
            color=POSITIVE,
            s=120,
            zorder=3,
            label="Average"
        )

        ax.scatter(
            df["Median"],
            y,
            color="orange",  
            marker='D',      # Diamond marker
            s=100,
            zorder=4,
            label="Median"
        )

        # Add labels for Min, Avg, Max
        for i, row in df.iterrows():
            # ax.text(row["Min"], i + 0.1, f'{row["Min"]:.1f}%', ha='right', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Avg"], i + 0.1, f'{row["Avg"]:.1f}%', ha='center', va='bottom', fontsize=10, color=POSITIVE, fontweight='bold')
            # ax.text(row["Max"], i + 0.1, f'{row["Max"]:.1f}%', ha='left', va='bottom', fontsize=10, color=NEGATIVE)
            ax.text(row["Median"], i - 0.15, f'{row["Median"]:.1f}%', ha='center', va='top', fontsize=10, color='orange', fontweight='bold')

        ax.set_yticks(y)
        ax.set_yticklabels(df["Typology"])
        ax.set_xlabel("No-data Fraction (%)")
        ax.set_title("No-data Fraction (Min–Avg–Max)", fontweight="bold")
        ax.grid(axis="x", alpha=0.3)
        ax.legend()

        plt.tight_layout()
        plt.savefig(output_dir / "no_data_fraction_range.png", dpi=300)
        plt.close()

    

    density_rows = []

    for typ, stats in all_stats.items():
        if stats.get("rf_pt_density_avg") is not None:
            density_rows.append({
                "Typology": typ,
                "Min": stats["rf_pt_density_min"],
                "Avg": stats["rf_pt_density_avg"],
                "Max": stats["rf_pt_density_max"],
                "Median": stats["rf_pt_density_median"],
            })

    if density_rows:
        df = pd.DataFrame(density_rows)

        fig, ax = plt.subplots(figsize=(10, 6))

        x = np.arange(len(df))
        ax.hlines(
            y=x,
            xmin=df["Min"],
            xmax=df["Max"],
            color=NEGATIVE,
            linewidth=4,
            label="Min–Max range"
        )

        # Plot Average
        ax.scatter(
            df["Avg"],
            x,
            color=POSITIVE,
            s=120,
            zorder=3,
            label="Average"
        )

        # Plot Median with a distinct marker
        
        ax.scatter(
            df["Median"],
            x,
            color="orange",  
            marker='D',      # Diamond marker
            s=100,
            zorder=4,
            label="Median"
        )


        # Add labels for Avg and Median
        for i, row in df.iterrows():
            ax.text(
                row["Avg"], i + 0.1, f'{row["Avg"]:.1f}',
                ha='center', va='bottom',
                fontsize=10, color=POSITIVE, fontweight='bold'
            )
            ax.text(
                row["Median"], i - 0.15, f'{row["Median"]:.1f}',
                ha='center', va='top',
                fontsize=10, color='orange', fontweight='bold'
            )

        ax.set_yticks(x)
        ax.set_yticklabels(df["Typology"])
        ax.set_xlabel("Points per m²")
        ax.set_title("Point Cloud Density (Min–Avg–Max)", fontweight="bold")
        ax.grid(axis="x", alpha=0.3)
        ax.legend()

        plt.tight_layout()
        plt.savefig(output_dir / "point_density_range.png", dpi=300)
        plt.close()

    
    # ---------------------------------------------------------
    # Radar chart (polygon grid, 60–100%) - One per typology
    # ---------------------------------------------------------
    categories = [
        "No-overlap",
        "Valid Geometry",
        "Extrusion Successful",
        "Usable Point Cloud",
        "Quality Indicator Positive",
    ]
    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]

    # Color palette
    colors = ["#FABF1A", "#72A0AA", "#095256", "#904E55", "#F17105"]

    for i, (typ, stats) in enumerate(all_stats.items()):
        fig, ax = plt.subplots(figsize=(3, 3), subplot_kw=dict(polar=True))
        
        values = [
            (1 - stats.get("rf_overlapping_ratio", 0)) * 100,
            stats["rf_val3dity_valid_ratio"] * 100,
            (1 - stats["rf_extrusion_skip_ratio"]) * 100,
            (1 - stats["rf_pointcloud_unusable_ratio"]) * 100,
            stats.get("rf_qualityindicator_ratio", 0) * 100,
        ]
        values += values[:1]
        
  
        ax.plot(angles, values, linewidth=2, label=typ.title(), color="#72A0AA")
        ax.fill(angles, values, alpha=0.4, color="#72A0AA")  
        #         
        ax.set_ylim(30, 100)


        ax.set_yticks([30, 40, 50, 60, 70, 80, 90, 100])

        ax.set_yticklabels([30, 40, 50, 60, 70, 80, 90, 100], color='grey', size=4.5)
        ax.yaxis.grid(True, linestyle="-")
        ax.xaxis.grid(True)
        
        # Make outer circle grey
        ax.spines['polar'].set_color('grey')
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, size=8)
        
        # Adjust label position to avoid collision
        ax.tick_params(axis='x', pad=10)
        
        plt.title(f"Overall Quality Radar – {typ.title()}", fontweight="bold", pad=10)
        plt.savefig(output_dir / f"{typ}_quality_radar.png", dpi=300, bbox_inches="tight")
        plt.close()

    logging.info("Redesigned visualizations saved successfully.")

        
def main() -> None:

    logging.info("Starting CityJSON quality analysis with improved data parsing...")
    safe_mkdir(OUTPUT_BASE_DIR)
    safe_mkdir(OUTPUT_STATS_DIR)
    safe_mkdir(OUTPUT_PLOTS_DIR)
    
    # Find all city.json files
    cityjson_files = list(DEST_DIR.glob("*.city.json"))
    logging.info(f"Found {len(cityjson_files)} CityJSON files")
    
    # Collect all buildings per typology across all tiles
    typology_buildings = defaultdict(list)
    typology_files = defaultdict(list)
    
    # Debug: Track total buildings across all files
    total_buildings_in_files = 0
    
    for cityjson_path in tqdm(cityjson_files, desc="Loading CityJSON files"):
        parsed = parse_cityjson_filename(cityjson_path.name)
        if not parsed:
            logging.warning(f"Could not parse filename: {cityjson_path.name}")
            continue
        
        file_name, typology = parsed
        typology_files[typology].append((file_name, cityjson_path))
        
        logging.info(f"Loading {cityjson_path.name} (Typology: {typology})")
        
        # Load CityJSON
        cityjson_data = load_cityjson(cityjson_path)
        if not cityjson_data:
            continue
        
        # Count total buildings in this file
        city_objects = cityjson_data.get("CityObjects", {})
        buildings_in_file = sum(1 for obj in city_objects.values() if obj.get("type") == "Building")
        total_buildings_in_files += buildings_in_file
        logging.info(f" File contains {buildings_in_file} Building objects")
        
        # Extract buildings with improved parsing
        buildings_dict = extract_building_attributes(cityjson_data)
        if not buildings_dict:
            logging.warning(f"No buildings with FEATID found in {cityjson_path.name}")
            continue
        
        # Add to typology collection
        typology_buildings[typology].append(buildings_dict)
        logging.info(f" Extracted {len(buildings_dict)} buildings with FEATID")
    
    logging.info(f"Total Building objects across all files: {total_buildings_in_files}")
    
    # Aggregate all buildings per typology
    aggregated_buildings = aggregate_building_data(typology_buildings)
    
    # Calculate total aggregated buildings
    total_aggregated = sum(len(buildings) for buildings in aggregated_buildings.values())
    logging.info(f"Total aggregated buildings: {total_aggregated}")
    
    if total_buildings_in_files != total_aggregated:
        logging.warning(f"Building count mismatch: {total_buildings_in_files} in files vs {total_aggregated} aggregated")
    
    # Calculate aggregated statistics per typology
    all_stats = {}
    for typology, buildings in aggregated_buildings.items():
        logging.info(f"\nCalculating statistics for typology '{typology}' ({len(buildings)} buildings)")
        stats = calculate_statistics(buildings)
        all_stats[typology] = stats
    
    # Process GPKGs and calculate quality indicators
    typology_quality_stats = defaultdict(lambda: {"total_quality_pass": 0, "total_matched": 0,
    "total_overlapping": 0,  "total_features": 0})
    
    # Process GPKGs once per typology
    for typology in typology_buildings.keys():
        logging.info(f"\n{'='*60}")
        logging.info(f"Processing GPKGs for typology '{typology}'")
        logging.info(f"{'='*60}")
        
        # Create a master dict of all buildings for this typology
        master_buildings_dict = {}
        for buildings_dict in typology_buildings[typology]:
            master_buildings_dict.update(buildings_dict)
        logging.info(f"Master dict contains {len(master_buildings_dict)} buildings")
        
        # Normalize typology to match filenames
        typology_key = typology_to_key(typology)
        
        # Find all GPKG files for this typology
        gpkg_candidates = list(
            DEST_DIR.glob(f"*_{typology_key}_{FOOTPRINTS_BASE_NAME}.gpkg")
        )
        
        if not gpkg_candidates:
            logging.warning(
                f"No GPKG found for typology '{typology}' "
                f"(pattern: *_{typology_key}_{FOOTPRINTS_BASE_NAME}.gpkg)"
            )
            continue
        
        logging.info(f"Found {len(gpkg_candidates)} GPKG file(s) for typology '{typology}'")
        
        # Process each GPKG file ONCE
        for gpkg_path in gpkg_candidates:
            logging.info(f"\nProcessing GPKG: {gpkg_path.name}")
            
            # Calculate overlaps
            gdf = calculate_footprint_overlaps(gpkg_path)


            # Calculate overlap statistics
            overlap_stats = calculate_overlap_statistics_from_gpkg(gdf)
            typology_quality_stats[typology]["total_overlapping"] += overlap_stats["rf_overlapping_count"]
            typology_quality_stats[typology]["total_features"] += len(gdf)
            
            # Calculate quality indicator using master buildings dict
            gdf = calculate_quality_indicator(gdf, master_buildings_dict)
            
            # Create backup before overwriting
            backup_path = gpkg_path.with_suffix(".gpkg.bak")
            if not backup_path.exists():
                import shutil
                shutil.copy2(gpkg_path, backup_path)
                logging.info(f"Created backup: {backup_path.name}")
            else:
                logging.info(f"Backup already exists: {backup_path.name}")
            
            # Save updated GPKG
            gdf.to_file(gpkg_path, driver="GPKG")
            logging.info(f"Updated GPKG saved: {gpkg_path.name}")
            
            # Accumulate quality stats for this typology
            quality_count = gdf["rf_qualityindicator"].sum()
            matched_count = gdf["rf_val3dity_lod22_valid"].notna().sum()
            typology_quality_stats[typology]["total_quality_pass"] += quality_count
            typology_quality_stats[typology]["total_matched"] += matched_count
            
            logging.info(f"  Quality pass: {quality_count}/{matched_count} features")
    
    # Add quality indicator stats to aggregated statistics
    for typology, quality_stats in typology_quality_stats.items():
        total_quality = quality_stats["total_quality_pass"]
        total_matched = quality_stats["total_matched"]
        quality_ratio = total_quality / total_matched if total_matched > 0 else 0

        # Calculate overlapping statistics
        total_overlapping = quality_stats["total_overlapping"]
        total_features = quality_stats["total_features"]
        overlapping_ratio = total_overlapping / total_features if total_features > 0 else 0
    
        all_stats[typology]["rf_qualityindicator_count"] = int(total_quality)
        all_stats[typology]["rf_qualityindicator_ratio"] = float(quality_ratio)
        all_stats[typology]["rf_matched_features"] = int(total_matched)

        # Calculate overlapping statistics
        all_stats[typology]["rf_overlapping_count"] = int(total_overlapping)
        all_stats[typology]["rf_overlapping_ratio"] = float(overlapping_ratio)
        all_stats[typology]["rf_total_features"] = int(total_features)
    
    # Save aggregated statistics to JSON
    stats_output = OUTPUT_STATS_DIR / "aggregated_statistics_by_typology.json"
    with open(stats_output, 'w', encoding='utf-8') as f:
        json.dump(all_stats, f, indent=2, default=str)
    logging.info(f"Statistics saved to {stats_output}")
    
    # Create summary CSV
    summary_rows = []
    for typology, stats in all_stats.items():
        row = {"Typology": typology}
        row.update(stats)
        summary_rows.append(row)
    
    df_summary = pd.DataFrame(summary_rows)
    summary_csv = OUTPUT_STATS_DIR / "summary_statistics.csv"
    df_summary.to_csv(summary_csv, index=False)
    logging.info(f"Summary CSV saved to {summary_csv}")
    
    # Create visualizations
    if all_stats:
        create_typology_visualizations(all_stats, OUTPUT_PLOTS_DIR)
    
    logging.info("Analysis complete!")
    
    # Print summary
    print("\n" + "="*80)
    print("QUALITY ANALYSIS SUMMARY")
    print("="*80)
    
    for typology in sorted(all_stats.keys()):
        stats = all_stats[typology]
        num_files = len(typology_files[typology])
        
        print(f"\n{typology.upper()} ({num_files} CityJSON tiles):")
        print(f"  Total Buildings: {stats['total_buildings']:,}")
        print(f"  Valid (val3dity): {stats['rf_val3dity_valid_ratio']:.1%}")
        
        if "rf_matched_features" in stats:
            print(f"  Matched Features: {stats['rf_matched_features']:,}")
        
        if "rf_qualityindicator_ratio" in stats:
            print(f"  Quality Indicator: {stats['rf_qualityindicator_ratio']:.1%}")
        
        if stats.get('rf_rmse_lod22_avg'):
            print(f"  Avg RMSE: {stats['rf_rmse_lod22_avg']:.3f}m")
            print(f"  Max RMSE: {stats['rf_rmse_lod22_max']:.3f}m")
        
        print(f"  Extrusion standard: {stats['rf_extrusion_standard_ratio']:.1%}")
        print(f"  LOD11 fallback:     {stats['rf_extrusion_lod11_fallback_ratio']:.1%}")
        print(f"  Skipped extrusion: {stats['rf_extrusion_skip_ratio']:.1%}")
        print(f"  Pointcloud Unusable: {stats['rf_pointcloud_unusable_ratio']:.1%}")

if __name__ == "__main__":
    
    main()